{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKtWMzIi8BRwpHRDFddEgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASEM000/PyTreeClass/blob/main/assets/benchmark_nn_training_flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytreeclass\n",
        "!pip install flax\n",
        "!pip install optax"
      ],
      "metadata": {
        "id": "SOfhiQD-jDk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import pytreeclass as pytc\n",
        "import flax\n",
        "import optax \n",
        "\n",
        "\n",
        "class PyTCLinear(pytc.TreeClass):\n",
        "    def __init__(self, in_dim: int, out_dim: int, key: jax.random.KeyArray, name: str):\n",
        "        self.name = name\n",
        "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
        "        self.bias = jax.numpy.array(0.0)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "\n",
        "def flax_linear(in_dim: int, out_dim: int, key: jax.random.KeyArray, name: str):\n",
        "    class FlaxLinear(flax.struct.PyTreeNode):\n",
        "        name: str = flax.struct.field(pytree_node=False)\n",
        "        weight: jax.Array\n",
        "        bias: jax.Array\n",
        "\n",
        "        def __call__(self, x: jax.Array):\n",
        "            return x @ self.weight + self.bias\n",
        "\n",
        "    return FlaxLinear(name, jax.random.normal(key, (in_dim, out_dim)), jax.numpy.array(0.0))\n",
        "\n",
        "\n",
        "def sequential_linears(layers, x):\n",
        "    *layers, last = layers\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "        x = jax.nn.relu(x)\n",
        "    return last(x)\n",
        "\n",
        "\n",
        "x = jnp.linspace(100, 1)[:, None]\n",
        "y = x**2\n",
        "key = jax.random.PRNGKey(0)\n",
        "optim = optax.adam(1e-3)\n",
        "\n",
        "\n",
        "@jax.value_and_grad\n",
        "def pytc_loss_func(layers,x,y):\n",
        "    layers = jax.tree_map(pytc.unfreeze, layers, is_leaf=pytc.is_frozen)\n",
        "    y = sequential_linears(layers, x)\n",
        "    return jnp.mean((x-y)**2)\n",
        "\n",
        "@jax.jit\n",
        "def pytc_train_step(layers, optim_state, x,y):\n",
        "    loss, grads = pytc_loss_func(layers,x,y)\n",
        "    updates, optim_state= optim.update(grads, optim_state)\n",
        "    layers = optax.apply_updates(layers, updates)\n",
        "    return layers, optim_state, loss\n",
        "\n",
        "@jax.value_and_grad\n",
        "def flax_loss_func(layers,x,y):\n",
        "    y = sequential_linears(layers, x)\n",
        "    return jnp.mean((x-y)**2)\n",
        "\n",
        "@jax.jit\n",
        "def flax_train_step(layers, optim_state, x,y):\n",
        "    loss, grads = flax_loss_func(layers,x,y)\n",
        "    updates, optim_state= optim.update(grads, optim_state)\n",
        "    layers = optax.apply_updates(layers, updates)\n",
        "    return layers, optim_state, loss\n",
        "\n",
        "\n",
        "def pytc_train(layers, optim_state, x,y, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        layers, optim_state, loss = pytc_train_step(layers,optim_state, x,y)\n",
        "    return layers, loss\n",
        "\n",
        "def flax_train(layers,optim_state, x,y, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        layers, optim_state, loss = flax_train_step(layers,optim_state, x,y)\n",
        "    return layers, loss\n",
        "\n",
        "\n",
        "for linear_count in [10, 100]:\n",
        "    pytc_linears = [PyTCLinear(1,1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\") for i in range(linear_count)]\n",
        "    # mask non-differentiable parameters\n",
        "    pytc_linears = jax.tree_map(lambda x: pytc.freeze(x) if pytc.is_nondiff(x) else x, pytc_linears)\n",
        "    pytc_optim_state = optim.init(pytc_linears)\n",
        "\n",
        "\n",
        "    flax_linears = [flax_linear(1,1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\") for i in range(linear_count)]\n",
        "    flax_optim_state = optim.init(flax_linears)\n",
        "\n",
        "\n",
        "    pytc_linears , pytc_loss = pytc_train(pytc_linears, pytc_optim_state, x,y, epochs=1000)\n",
        "    flax_linears, flax_loss = flax_train(flax_linears, flax_optim_state, x,y, epochs=1000)\n",
        "\n",
        "    assert pytc_loss == flax_loss\n",
        "\n",
        "    time_pytc = %timeit -o pytc_train(pytc_linears, pytc_optim_state, x,y, epochs=100)\n",
        "    time_flax = %timeit -o flax_train(flax_linears, flax_optim_state, x,y, epochs=100)\n",
        "    print(f\"Flax/PyTc: {time_flax.average/time_pytc.average} for {linear_count} layers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-26h47R721bp",
        "outputId": "93df8dee-0a1e-46b8-ed5c-46ec03a5ce42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.6 ms ± 548 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "30.8 ms ± 355 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "Flax/PyTc: 1.4270735299354067 for 10 layers\n",
            "474 ms ± 80.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "528 ms ± 41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Flax/PyTc: 1.113071349681521 for 100 layers\n"
          ]
        }
      ]
    }
  ]
}