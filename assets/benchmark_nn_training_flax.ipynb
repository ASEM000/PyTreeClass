{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOKtWMzIi8BRwpHRDFddEgl",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ASEM000/pytreeclass/blob/main/assets/benchmark_nn_training_flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pytreeclass\n",
    "!pip install flax\n",
    "!pip install optax"
   ],
   "metadata": {
    "id": "SOfhiQD-jDk4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pytreeclass as tc\n",
    "import flax\n",
    "import optax\n",
    "\n",
    "\n",
    "class tcLinear(tc.TreeClass):\n",
    "    def __init__(self, in_dim: int, out_dim: int, key: jax.random.KeyArray, name: str):\n",
    "        self.name = name\n",
    "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
    "        self.bias = jax.numpy.array(0.0)\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "def flax_linear(in_dim: int, out_dim: int, key: jax.random.KeyArray, name: str):\n",
    "    class FlaxLinear(flax.struct.PyTreeNode):\n",
    "        name: str = flax.struct.field(pytree_node=False)\n",
    "        weight: jax.Array\n",
    "        bias: jax.Array\n",
    "\n",
    "        def __call__(self, x: jax.Array):\n",
    "            return x @ self.weight + self.bias\n",
    "\n",
    "    return FlaxLinear(\n",
    "        name, jax.random.normal(key, (in_dim, out_dim)), jax.numpy.array(0.0)\n",
    "    )\n",
    "\n",
    "\n",
    "def sequential_linears(layers, x):\n",
    "    *layers, last = layers\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "        x = jax.nn.relu(x)\n",
    "    return last(x)\n",
    "\n",
    "\n",
    "x = jnp.linspace(100, 1)[:, None]\n",
    "y = x**2\n",
    "key = jax.random.PRNGKey(0)\n",
    "optim = optax.adam(1e-3)\n",
    "\n",
    "\n",
    "@jax.value_and_grad\n",
    "def tc_loss_func(layers, x, y):\n",
    "    layers = jax.tree_map(tc.unfreeze, layers, is_leaf=tc.is_frozen)\n",
    "    y = sequential_linears(layers, x)\n",
    "    return jnp.mean((x - y) ** 2)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def tc_train_step(layers, optim_state, x, y):\n",
    "    loss, grads = tc_loss_func(layers, x, y)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    layers = optax.apply_updates(layers, updates)\n",
    "    return layers, optim_state, loss\n",
    "\n",
    "\n",
    "@jax.value_and_grad\n",
    "def flax_loss_func(layers, x, y):\n",
    "    y = sequential_linears(layers, x)\n",
    "    return jnp.mean((x - y) ** 2)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def flax_train_step(layers, optim_state, x, y):\n",
    "    loss, grads = flax_loss_func(layers, x, y)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    layers = optax.apply_updates(layers, updates)\n",
    "    return layers, optim_state, loss\n",
    "\n",
    "\n",
    "def tc_train(layers, optim_state, x, y, epochs=100):\n",
    "    for _ in range(epochs):\n",
    "        layers, optim_state, loss = tc_train_step(layers, optim_state, x, y)\n",
    "    return layers, loss\n",
    "\n",
    "\n",
    "def flax_train(layers, optim_state, x, y, epochs=100):\n",
    "    for _ in range(epochs):\n",
    "        layers, optim_state, loss = flax_train_step(layers, optim_state, x, y)\n",
    "    return layers, loss\n",
    "\n",
    "\n",
    "for linear_count in [10, 100]:\n",
    "    tc_linears = [\n",
    "        tcLinear(1, 1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\")\n",
    "        for i in range(linear_count)\n",
    "    ]\n",
    "    # mask non-differentiable parameters\n",
    "    tc_linears = jax.tree_map(\n",
    "        lambda x: tc.freeze(x) if tc.is_nondiff(x) else x, tc_linears\n",
    "    )\n",
    "    tc_optim_state = optim.init(tc_linears)\n",
    "\n",
    "    flax_linears = [\n",
    "        flax_linear(1, 1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\")\n",
    "        for i in range(linear_count)\n",
    "    ]\n",
    "    flax_optim_state = optim.init(flax_linears)\n",
    "\n",
    "    tc_linears, tc_loss = tc_train(tc_linears, tc_optim_state, x, y, epochs=1000)\n",
    "    flax_linears, flax_loss = flax_train(\n",
    "        flax_linears, flax_optim_state, x, y, epochs=1000\n",
    "    )\n",
    "\n",
    "    assert tc_loss == flax_loss\n",
    "\n",
    "    time_tc = %timeit -o tc_train(tc_linears, tc_optim_state, x,y, epochs=100)\n",
    "    time_flax = %timeit -o flax_train(flax_linears, flax_optim_state, x,y, epochs=100)\n",
    "    print(f\"Flax/tc: {time_flax.average/time_tc.average} for {linear_count} layers\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-26h47R721bp",
    "outputId": "93df8dee-0a1e-46b8-ed5c-46ec03a5ce42"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21.6 ms ± 548 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "30.8 ms ± 355 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Flax/tc: 1.4270735299354067 for 10 layers\n",
      "474 ms ± 80.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "528 ms ± 41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Flax/tc: 1.113071349681521 for 100 layers\n"
     ]
    }
   ]
  }
 ]
}