{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0wKu0v3agovHvp6GUNTaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASEM000/pytreeclass/blob/main/assets/benchmark_nn_training_equinox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytreeclass\n",
        "!pip install equinox\n",
        "!pip install optax"
      ],
      "metadata": {
        "id": "SOfhiQD-jDk4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import pytreeclass as pytc\n",
        "import equinox as eqx\n",
        "import optax \n",
        "\n",
        "\n",
        "class PyTCLinear(pytc.TreeClass):\n",
        "    def __init__(self, in_dim: int, out_dim: int, key: jax.random.KeyArray, name: str):\n",
        "        self.name = name\n",
        "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
        "        self.bias = jax.numpy.array(0.0)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "\n",
        "class EqxLinear(eqx.Module):\n",
        "    name: str\n",
        "    weight: jax.Array\n",
        "    bias: jax.Array\n",
        "\n",
        "    def __init__(self, in_dim: int, out_dim: int, key: jax.random.KeyArray, name:str):\n",
        "        self.name = name\n",
        "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
        "        self.bias = jax.numpy.array(0.0)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "\n",
        "\n",
        "def sequential_linears(layers, x):\n",
        "    *layers, last = layers\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "        x = jax.nn.relu(x)\n",
        "    return last(x)\n",
        "\n",
        "\n",
        "x = jnp.linspace(100, 1)[:, None]\n",
        "y = x**2\n",
        "key = jax.random.PRNGKey(0)\n",
        "optim = optax.adam(1e-3)\n",
        "\n",
        "\n",
        "@jax.value_and_grad\n",
        "def pytc_loss_func(layers,x,y):\n",
        "    layers = jax.tree_map(pytc.unfreeze, layers, is_leaf=pytc.is_frozen)\n",
        "    y = sequential_linears(layers, x)\n",
        "    return jnp.mean((x-y)**2)\n",
        "\n",
        "@jax.jit\n",
        "def pytc_train_step(layers, optim_state, x,y):\n",
        "    loss, grads = pytc_loss_func(layers,x,y)\n",
        "    updates, optim_state= optim.update(grads, optim_state)\n",
        "    layers = optax.apply_updates(layers, updates)\n",
        "    return layers, optim_state, loss\n",
        "\n",
        "@eqx.filter_value_and_grad\n",
        "def eqx_loss_func(layers,x,y):\n",
        "    y = sequential_linears(layers, x)\n",
        "    return jnp.mean((x-y)**2)\n",
        "\n",
        "@eqx.filter_jit\n",
        "def eqx_train_step(layers, optim_state, x,y):\n",
        "    loss, grads = eqx_loss_func(layers,x,y)\n",
        "    updates, optim_state= optim.update(grads, optim_state)\n",
        "    layers = eqx.apply_updates(layers, updates)\n",
        "    return layers, optim_state, loss\n",
        "\n",
        "\n",
        "def pytc_train(layers, optim_state, x,y, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        layers, optim_state, loss = pytc_train_step(layers,optim_state, x,y)\n",
        "    return layers, loss\n",
        "\n",
        "def eqx_train(layers,optim_state, x,y, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        layers, optim_state, loss = eqx_train_step(layers,optim_state, x,y)\n",
        "    return layers, loss\n",
        "\n",
        "\n",
        "for linear_count in [10,100]:\n",
        "    pytc_linears = [PyTCLinear(1,1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\") for i in range(linear_count)]\n",
        "    # mask non-differentiable parameters\n",
        "    pytc_linears = jax.tree_map(lambda x: pytc.freeze(x) if pytc.is_nondiff(x) else x, pytc_linears)\n",
        "    pytc_optim_state = optim.init(pytc_linears)\n",
        "\n",
        "\n",
        "    eqx_linears = [EqxLinear(1,1, key=jax.random.PRNGKey(i), name=f\"linear_{i}\") for i in range(linear_count)]\n",
        "    eqx_optim_state = optim.init(eqx.filter(eqx_linears, eqx.is_array))\n",
        "\n",
        "\n",
        "    pytc_linears , pytc_loss = pytc_train(pytc_linears, pytc_optim_state, x,y, epochs=1000)\n",
        "    eqx_linears, eqx_loss = eqx_train(eqx_linears, eqx_optim_state, x,y, epochs=1000)\n",
        "\n",
        "    assert pytc_loss == eqx_loss\n",
        "\n",
        "    time_pytc = %timeit -o pytc_train(pytc_linears, pytc_optim_state, x,y, epochs=100)\n",
        "    time_eqx = %timeit -o eqx_train(eqx_linears, eqx_optim_state, x,y, epochs=100)\n",
        "    print(f\"Eqx/PyTc: {time_eqx.average/time_pytc.average} for {linear_count} layers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-26h47R721bp",
        "outputId": "3331878c-ad0e-41af-c83c-c8a99707ff38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.4 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "230 ms ± 93.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "Eqx/PyTc: 6.671167451529536 for 10 layers\n",
            "659 ms ± 19.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "1.79 s ± 272 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Eqx/PyTc: 2.714461166827432 for 100 layers\n"
          ]
        }
      ]
    }
  ]
}