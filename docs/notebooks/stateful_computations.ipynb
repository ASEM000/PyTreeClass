{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“œ Stateful computations<a id=\"StatefulComputation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to handle internal states in the immutable `pytreeclass` with functional API.\n",
    "\n",
    "First, [Under jax.jit jax requires states to be explicit](https://jax.readthedocs.io/en/latest/jax-101/07-state.html?highlight=state), this means that for any class instance; variables needs to be separated from the class and be passed explictly. However when using @pytc.treeclass no need to separate the instance variables ; instead the whole instance is passed as a state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a concrete example, the following code demonstrate the difference between the _Explicit parameters approach_ and the _class instance state as parameter approach_\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr><td align=\"center\">Passing explicit parameters</td><td align=\"center\">Passing state as parameter</td></tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "```python\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.nn as jnn\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "def init_params(layers):\n",
    "  keys = jr.split(jr.PRNGKey(0),len(layers)-1)\n",
    "  params = list()\n",
    "  init_func = jnn.initializers.he_normal()\n",
    "  \n",
    "  for key,n_in,n_out in zip(\n",
    "    keys,layers[:-1],layers[1:]\n",
    "  ):\n",
    "    W = init_func(key,(n_in,n_out))\n",
    "    B = jr.uniform(key,shape=(n_out,))\n",
    "    params.append({'W':W,'B':B})\n",
    "  return params\n",
    "\n",
    "def fwd(params,x):\n",
    "  *hidden,last = params\n",
    "  for layer in hidden :\n",
    "    x = jnn.tanh(x@layer['W']+layer['B'])\n",
    "  return x@last['W'] + last['B']\n",
    "\n",
    "@jax.value_and_grad\n",
    "def loss_func(params,x,y):\n",
    "  pred = fwd(params,x)\n",
    "  return jnp.mean((pred-y)**2)\n",
    "\n",
    "@jax.jit\n",
    "def update(params,x,y):\n",
    "  # gradient w.r.t to params\n",
    "  value,grads= loss_func(params,x,y)\n",
    "  sgd = lambda p,g: p-1e-3*g\n",
    "  params = jtu.tree_map(sgd, params,grads)\n",
    "  return value,params\n",
    "\n",
    "x = jnp.linspace(0,1,100).reshape(100,1)\n",
    "y = x**2 -1 \n",
    "\n",
    "params = init_params([1] +[5]*4+[1] )\n",
    "\n",
    "epochs = 10_000\n",
    "for _ in range(1,epochs+1):\n",
    "  value , params = update(params,x,y)\n",
    "\n",
    "  # print loss and epoch info\n",
    "  if _ %(1_000) ==0:\n",
    "    print(f'Epoch={_}\\tloss={value:.3e}')\n",
    "```\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "\n",
    "<td>\n",
    "\n",
    "```python\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import jax.random as jr\n",
    "import jax.nn as jnn\n",
    "import jax.tree_util as jtu\n",
    "import pytreeclass as pytc\n",
    "\n",
    "\n",
    "class MLP(pytc.TreeClass):\n",
    "  Layers : list\n",
    "\n",
    "  def __init__(self,layers):\n",
    "    keys = jr.split(jr.PRNGKey(0),len(layers)-1)\n",
    "    self.Layers = list()\n",
    "    init_func = jnn.initializers.he_normal()\n",
    "    \n",
    "    for key,n_in,n_out in zip(\n",
    "      keys,layers[:-1],layers[1:]\n",
    "    ):\n",
    "      W = init_func(key,(n_in,n_out))\n",
    "      B = jr.uniform(key,shape=(n_out,))\n",
    "      self.Layers.append({'W':W,'B':B})\n",
    "    \n",
    "  def __call__(self,x):\n",
    "    *hidden,last = self.Layers\n",
    "    for layer in hidden :\n",
    "      x = jnn.tanh(x@layer['W']+layer['B'])\n",
    "    return x@last['W'] + last['B'] \n",
    "\n",
    "@jax.value_and_grad\n",
    "def loss_func(model,x,y):\n",
    "  pred = model(x)\n",
    "  return jnp.mean((pred-y)**2)\n",
    "\n",
    "@jax.jit\n",
    "def update(model,x,y):\n",
    "  # gradient w.r.t to model\n",
    "  value , grads= loss_func(model,x,y)\n",
    "  model = model-1e-3*grads\n",
    "  return value , model\n",
    "\n",
    "x = jnp.linspace(0,1,100).reshape(100,1)\n",
    "y = x**2 -1 \n",
    "\n",
    "model = MLP([1] +[5]*4+[1] )\n",
    "\n",
    "epochs = 10_000\n",
    "for _ in range(1,epochs+1):\n",
    "  value , model = update(model,x,y)\n",
    "\n",
    "  # print loss and epoch info\n",
    "  if _ %(1_000) ==0:\n",
    "    print(f'Epoch={_}\\tloss={value:.3e}')\n",
    "```\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the following pattern,Updating state **functionally** can be achieved under `jax.jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import pytreeclass as pytc\n",
    "\n",
    "\n",
    "class Counter(pytc.TreeClass):\n",
    "    calls: int = 0\n",
    "\n",
    "    def increment(self):\n",
    "        self.calls += 1\n",
    "\n",
    "\n",
    "counter = Counter()  # Counter(calls=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the update function. Since the increment method mutate the internal state, thus we need to use the functional approach to update the state by using `.at`. To achieve this we can use `.at[method_name].__call__(*args,**kwargs)`, this functional call will return the value of this call and a _new_ model instance with the update state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def update(counter):\n",
    "    # update the function functionally\n",
    "    # return the call output value and\n",
    "    # the updated model\n",
    "    value, new_counter = counter.at[\"increment\"]()\n",
    "    return new_counter\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    counter = update(counter)\n",
    "\n",
    "print(counter.calls)  # 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dev-jax15')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2499512ed2537f1f7868337b11978106d2ebb2fc6ab5814ee177db416ca4ea91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
