{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete reference :** \n",
    "https://jax.readthedocs.io/en/latest/jax-101/07-state.html?highlight=state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem however is that jax requires states to be explicit, this means that the class instance variables needs to be separated from the class and be passed explictly.\n",
    "However when using `@treeclass` no need to separate the instance variables ; instead the whole instance is passed as a state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>Explicit State</td><td>Instance as a state</td></tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "```python\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "\n",
    "def init_params(layers):\n",
    "    keys = jax.random.split(\n",
    "        jax.random.PRNGKey(0),len(layers)-1\n",
    "    )\n",
    "    params = list()\n",
    "    init_func = jax.nn.initializers.he_normal()\n",
    "    for key,n_in,n_out in zip(keys,layers[:-1],layers[1:]):\n",
    "        W = init_func(key,(n_in,n_out))\n",
    "        B = jax.random.uniform(key,shape=(n_out,))\n",
    "        params.append({'W':W,'B':B})\n",
    "    return params\n",
    "\n",
    "def fwd(params,x):\n",
    "    *hidden,last = params\n",
    "    for layer in hidden :\n",
    "        x = jax.nn.tanh(x@layer['W']+layer['B'])\n",
    "    return x@last['W'] + last['B']\n",
    "\n",
    "def loss_func(params,x,y):\n",
    "    pred = fwd(params,x)\n",
    "    return jnp.mean((pred-y)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params,x,y):\n",
    "    # gradient w.r.t to params\n",
    "    value,grads= jax.value_and_grad(loss_func,0)(params,x,y)\n",
    "    params =  jax.tree_map(\n",
    "        lambda params,grads : params-1e-3*grads, params,grads\n",
    "    )\n",
    "    return value,params\n",
    "\n",
    "x = jnp.linspace(0,1,100).reshape(100,1)\n",
    "y = x**2 -1 \n",
    "\n",
    "params = init_params([1] +[5]*4+[1] )\n",
    "\n",
    "epochs = 10_000\n",
    "for _ in range(epochs):\n",
    "    value , params = update(params,x,y)\n",
    "\n",
    "    # print loss and epoch info\n",
    "    if _ %(1_000) ==0:\n",
    "        print(f'Epoch={_}\\tloss={value:.3e}')\n",
    "```\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "```python\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "\n",
    "@treeclass\n",
    "class MLP:\n",
    "    Layers : list\n",
    "\n",
    "    def __init__(self,layers):\n",
    "        keys = jax.random.split(\n",
    "            jax.random.PRNGKey(0),len(layers)-1\n",
    "        )\n",
    "        self.Layers = list()\n",
    "        init_func = jax.nn.initializers.he_normal()\n",
    "        for key,n_in,n_out in zip(keys,layers[:-1],layers[1:]):\n",
    "            lb, ub = -(1 / jnp.sqrt(n_in)), (1 / jnp.sqrt(n_in))\n",
    "            W = init_func(key,(n_in,n_out))\n",
    "            B = jax.random.uniform(key,shape=(n_out,))\n",
    "            self.Layers.append({'W':W,'B':B})\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        *hidden,last = self.Layers\n",
    "        for layer in hidden :\n",
    "            x = jax.nn.tanh(x@layer['W']+layer['B'])\n",
    "        return x@last['W'] + last['B'] \n",
    "\n",
    "def loss_func(model,x,y):\n",
    "    pred = model(x)\n",
    "    return jnp.mean((pred-y)**2)\n",
    "\n",
    "@jax.jit\n",
    "def update(model,x,y):\n",
    "    # gradient w.r.t to model\n",
    "    value , grads= jax.value_and_grad(loss_func,0)(model,x,y)\n",
    "    model = jax.tree_map(\n",
    "        lambda model,grads : model-1e-3*grads, model,grads\n",
    "    )\n",
    "    return value , model\n",
    "\n",
    "x = jnp.linspace(0,1,100).reshape(100,1)\n",
    "y = x**2 -1 \n",
    "\n",
    "model = MLP([1] +[5]*4+[1] )\n",
    "\n",
    "epochs = 10_000\n",
    "for _ in range(epochs):\n",
    "    value , model = update(model,x,y)\n",
    "\n",
    "    # print loss and epoch info\n",
    "    if _ %(1_000) ==0:\n",
    "        print(f'Epoch={_}\\tloss={value:.3e}')\n",
    "```\n",
    "\n",
    "</td>\n",
    "\n",
    "\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('dev-jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dbabdc3ed9c56a6e246ce2e910e119890ccd2806df70655d281d35df3853255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
