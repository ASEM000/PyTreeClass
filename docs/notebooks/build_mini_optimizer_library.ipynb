{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§® Building a mini optimizer library\n",
    "\n",
    "In this notebook stateful Exponential moving average  (`EMA`) and `Adam` is built using `PyTreeClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam(mu=Tree(a=2.0), nu=Tree(a=14.4), count=1)\n",
      "Tree(a=3.0)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pytreeclass as pytc\n",
    "\n",
    "\n",
    "class Tree(pytc.TreeClass):\n",
    "    a: float\n",
    "\n",
    "\n",
    "def loss_func(tree: Tree, x):\n",
    "    return tree.a**x\n",
    "\n",
    "\n",
    "def _moment_update(grads, moments, *, beta: float, order: int):\n",
    "    def _moment_step(grad, moment):\n",
    "        return beta * moment + (1 - beta) * (grad**order)\n",
    "\n",
    "    return jax.tree_map(_moment_step, grads, moments)\n",
    "\n",
    "\n",
    "def _debias_update(moments, *, beta: float, count: int):\n",
    "    def _debias_step(moment):\n",
    "        return moment / (1 - beta**count)\n",
    "\n",
    "    return jax.tree_map(_debias_step, moments)\n",
    "\n",
    "\n",
    "def ema(decay_rate: float, debias: float = True):\n",
    "    \"\"\"Exponential moving average\n",
    "\n",
    "    Args:\n",
    "        decay_rate: The decay rate of the moving average.\n",
    "        debias: Whether to debias the moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    class EMA(pytc.TreeClass):\n",
    "        def __init__(self, tree):\n",
    "            self.state = jax.tree_map(jnp.zeros_like, tree)\n",
    "            self.count = 0\n",
    "\n",
    "        def _update(self, grads):\n",
    "            self.count += 1\n",
    "            self.state = _moment_update(\n",
    "                grads,\n",
    "                self.state,\n",
    "                beta=decay_rate,\n",
    "                order=1,\n",
    "            )\n",
    "            if debias:\n",
    "                return _debias_update(\n",
    "                    self.state,\n",
    "                    beta=decay_rate,\n",
    "                    count=self.count,\n",
    "                )\n",
    "            return self.state\n",
    "\n",
    "        def update(self, grads):\n",
    "            return self.at[\"_update\"](grads)\n",
    "\n",
    "    return EMA\n",
    "\n",
    "\n",
    "def adam(*, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n",
    "    \"\"\"Adam optimizer\n",
    "\n",
    "    Args:\n",
    "        beta1: The decay rate of the first moment.\n",
    "        beta2: The decay rate of the second moment.\n",
    "        eps: A small value to prevent division by zero.\n",
    "    \"\"\"\n",
    "\n",
    "    class Adam(pytc.TreeClass):\n",
    "        def __init__(self, tree):\n",
    "            self.mu = jax.tree_map(jnp.zeros_like, tree)\n",
    "            self.nu = jax.tree_map(jnp.zeros_like, tree)\n",
    "            self.count = 0\n",
    "\n",
    "        def _update(self, grads):\n",
    "            self.count += 1\n",
    "            self.mu = _moment_update(\n",
    "                grads,\n",
    "                self.mu,\n",
    "                beta=beta1,\n",
    "                order=1,\n",
    "            )\n",
    "            self.nu = _moment_update(\n",
    "                grads,\n",
    "                self.nu,\n",
    "                beta=beta2,\n",
    "                order=2,\n",
    "            )\n",
    "\n",
    "            mu_hat = _debias_update(\n",
    "                self.mu,\n",
    "                beta=beta1,\n",
    "                count=self.count,\n",
    "            )\n",
    "            nu_hat = _debias_update(\n",
    "                self.nu,\n",
    "                beta=beta2,\n",
    "                count=self.count,\n",
    "            )\n",
    "\n",
    "            return jax.tree_map(\n",
    "                lambda mu, nu: mu / (jnp.sqrt(nu) + eps), mu_hat, nu_hat\n",
    "            )\n",
    "\n",
    "        def update(self, grads):\n",
    "            # since self._update mutates the state, we need to use self.at\n",
    "            # to return the method value and the mutated state\n",
    "            return self.at[\"_update\"](grads)\n",
    "\n",
    "    return Adam\n",
    "\n",
    "\n",
    "tree = Tree(a=2.0)\n",
    "grads = jax.grad(loss_func)(tree, 2.0)\n",
    "optim = adam(beta1=0.5, beta2=0.1)\n",
    "optim_state = optim(tree)\n",
    "\n",
    "updates, optim_state = optim_state.update(grads)\n",
    "print(optim_state)\n",
    "\n",
    "# updated tree\n",
    "tree = jax.tree_map(lambda x, y: x + y, tree, updates)\n",
    "print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
